{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa40e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‚ Fold 1\n",
      "ðŸ”§ Pretraining GMF...\n",
      "ðŸ“Š Fold 1: GMF: Loss=0.9829, RMSE=0.9914\n",
      "ðŸ”§ Pretraining MLP...\n",
      "ðŸ“Š Fold 1: MLP: Loss=0.8951, RMSE=0.9461\n",
      "ðŸ§  Fine-tuning NeuMF (NCF)...\n",
      "ðŸ“Š Fold 1: Loss=0.8739, RMSE=0.9348\n",
      "\n",
      "ðŸ“‚ Fold 2\n",
      "ðŸ”§ Pretraining GMF...\n",
      "ðŸ“Š Fold 2: GMF: Loss=0.9265, RMSE=0.9625\n",
      "ðŸ”§ Pretraining MLP...\n",
      "ðŸ“Š Fold 2: MLP: Loss=0.8888, RMSE=0.9427\n",
      "ðŸ§  Fine-tuning NeuMF (NCF)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m ncf_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(ncf\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ§  Fine-tuning NeuMF (NCF)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 63\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mncf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mncf_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mncf_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m loss, rmse \u001b[38;5;241m=\u001b[39m evaluation(ncf, test_loader, ncf_criterion, device)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ“Š Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, RMSE=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\HCMUT_Workspace\\HK242\\Multidisciplinary-Project\\Recommendation-System-for-MovieLens\\src\\NCF\\utils.py:42\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[0;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, ratings)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Calculate gradient descent\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Update weight\u001b[39;00m\n\u001b[0;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\T14s G4\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\T14s G4\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\T14s G4\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import RatingDataset, train, evaluation\n",
    "from GMF import GMF\n",
    "from MLP import MLP\n",
    "from NCF import NeuralCF  # make sure this points to your NeuralCF definition\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 0.001\n",
    "n_users, n_items = 943, 1682\n",
    "gmf_factors = 64\n",
    "mlp_layers = [128, 16, 8]\n",
    "epochs = 10\n",
    "\n",
    "for i in range(1, 6):\n",
    "    print(f\"\\nðŸ“‚ Fold {i}\")\n",
    "\n",
    "    train_path = f'../../data/ml-100k/u{i}.base'\n",
    "    test_path = f'../../data/ml-100k/u{i}.test'\n",
    "\n",
    "    train_dataset = RatingDataset(train_path)\n",
    "    test_dataset = RatingDataset(test_path)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # === PRETRAIN GMF ===\n",
    "    gmf = GMF(n_users, n_items, gmf_factors).to(device)\n",
    "    gmf_optimizer = torch.optim.Adam(gmf.parameters(), lr=learning_rate)\n",
    "    gmf_criterion = nn.MSELoss()\n",
    "\n",
    "    print(\"ðŸ”§ Pretraining GMF...\")\n",
    "    train(gmf, train_loader, gmf_criterion, gmf_optimizer, device)\n",
    "    loss, rmse = evaluation(gmf, test_loader, gmf_criterion, device)\n",
    "    print(f\"ðŸ“Š Fold {i}: GMF: Loss={loss:.4f}, RMSE={rmse:.4f}\")\n",
    "    torch.save(gmf.state_dict(), f\"./pretrain/gmf_fold{i}.pt\")\n",
    "\n",
    "    # === PRETRAIN MLP ===\n",
    "    mlp = MLP(n_users, n_items, mlp_layers).to(device)\n",
    "    mlp_optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate)\n",
    "    mlp_criterion = nn.MSELoss()\n",
    "\n",
    "    print(\"ðŸ”§ Pretraining MLP...\")\n",
    "    train(mlp, train_loader, mlp_criterion, mlp_optimizer, device)\n",
    "    loss, rmse = evaluation(mlp, test_loader, mlp_criterion, device)\n",
    "    print(f\"ðŸ“Š Fold {i}: MLP: Loss={loss:.4f}, RMSE={rmse:.4f}\")\n",
    "    torch.save(mlp.state_dict(), f\"./pretrain/mlp_fold{i}.pt\")\n",
    "\n",
    "    # === LOAD PRETRAINED & BUILD NCF ===\n",
    "    pretrained_gmf = GMF(n_users, n_items, gmf_factors, for_NeuMF=False).to(device)\n",
    "    pretrained_mlp = MLP(n_users, n_items, mlp_layers, for_NeuMF=False).to(device)\n",
    "    pretrained_gmf.load_state_dict(torch.load(f\"./pretrain/gmf_fold{i}.pt\"))\n",
    "    pretrained_mlp.load_state_dict(torch.load(f\"./pretrain/mlp_fold{i}.pt\"))\n",
    "\n",
    "    ncf = NeuralCF(n_users, n_items, gmf_factors, mlp_layers,\n",
    "                   GMF_model=pretrained_gmf, MLP_model=pretrained_mlp).to(device)\n",
    "\n",
    "    ncf_criterion = nn.MSELoss()\n",
    "    ncf_optimizer = torch.optim.Adam(ncf.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(\"ðŸ§  Fine-tuning NeuMF (NCF)...\")\n",
    "    train(ncf, train_loader, ncf_criterion, ncf_optimizer, device)\n",
    "\n",
    "    loss, rmse = evaluation(ncf, test_loader, ncf_criterion, device)\n",
    "    print(f\"ðŸ“Š Fold {i}: Loss={loss:.4f}, RMSE={rmse:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
