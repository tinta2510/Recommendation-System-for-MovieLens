{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa40e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import RatingDataset, train, evaluation\n",
    "from GMF import GMF\n",
    "from MLP import MLP\n",
    "from NCF import NeuralCF  # make sure this points to your NeuralCF definition\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 0.001\n",
    "n_users, n_items = 943, 1682\n",
    "gmf_factors = 8\n",
    "mlp_layers = [32, 16, 8]\n",
    "epochs = 10\n",
    "\n",
    "for i in range(1, 6):\n",
    "    print(f\"\\nðŸ“‚ Fold {i}\")\n",
    "\n",
    "    train_path = f'../../data/ml-100k/u{i}.base'\n",
    "    test_path = f'../../data/ml-100k/u{i}.test'\n",
    "\n",
    "    train_dataset = RatingDataset(train_path)\n",
    "    test_dataset = RatingDataset(test_path)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # === PRETRAIN GMF ===\n",
    "    gmf = GMF(n_users, n_items, gmf_factors).to(device)\n",
    "    gmf_optimizer = torch.optim.Adam(gmf.parameters(), lr=learning_rate)\n",
    "    gmf_criterion = nn.MSELoss()\n",
    "\n",
    "    print(\"ðŸ”§ Pretraining GMF...\")\n",
    "    train(gmf, train_loader, gmf_criterion, gmf_optimizer, device)\n",
    "    loss, rmse = evaluation(gmf, test_loader, gmf_criterion, device)\n",
    "    print(f\"ðŸ“Š Fold {i}: GMF: Loss={loss:.4f}, RMSE={rmse:.4f}\")\n",
    "    torch.save(gmf.state_dict(), f\"./pretrain/gmf_fold{i}.pt\")\n",
    "\n",
    "    # === PRETRAIN MLP ===\n",
    "    mlp = MLP(n_users, n_items, mlp_layers).to(device)\n",
    "    mlp_optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate)\n",
    "    mlp_criterion = nn.MSELoss()\n",
    "\n",
    "    print(\"ðŸ”§ Pretraining MLP...\")\n",
    "    train(mlp, train_loader, mlp_criterion, mlp_optimizer, device)\n",
    "    loss, rmse = evaluation(mlp, test_loader, mlp_criterion, device)\n",
    "    print(f\"ðŸ“Š Fold {i}: MLP: Loss={loss:.4f}, RMSE={rmse:.4f}\")\n",
    "    torch.save(mlp.state_dict(), f\"./pretrain/mlp_fold{i}.pt\")\n",
    "\n",
    "    # === LOAD PRETRAINED & BUILD NCF ===\n",
    "    pretrained_gmf = GMF(n_users, n_items, gmf_factors, for_NeuMF=False).to(device)\n",
    "    pretrained_mlp = MLP(n_users, n_items, mlp_layers, for_NeuMF=False).to(device)\n",
    "    pretrained_gmf.load_state_dict(torch.load(f\"./pretrain/gmf_fold{i}.pt\"))\n",
    "    pretrained_mlp.load_state_dict(torch.load(f\"./pretrain/mlp_fold{i}.pt\"))\n",
    "\n",
    "    ncf = NeuralCF(n_users, n_items, gmf_factors, mlp_layers,\n",
    "                   GMF_model=pretrained_gmf, MLP_model=pretrained_mlp).to(device)\n",
    "\n",
    "    ncf_criterion = nn.MSELoss()\n",
    "    ncf_optimizer = torch.optim.Adam(ncf.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(\"ðŸ§  Fine-tuning NeuMF (NCF)...\")\n",
    "    train(ncf, train_loader, ncf_criterion, ncf_optimizer, device)\n",
    "\n",
    "    loss, rmse = evaluation(ncf, test_loader, ncf_criterion, device)\n",
    "    print(f\"ðŸ“Š Fold {i}: Loss={loss:.4f}, RMSE={rmse:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
